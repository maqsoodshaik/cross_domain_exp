# -*- coding: utf-8 -*-
"""thesis_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AwvpRQc1-kX4VvjFVREjmVFaPMe754jk
"""



from turtle import color
import torch
model_checkpoint = "wav2vec2-basefrenchgermandutchmultilingual_librispeech_bestmodel"
batch_size = 32
from os import rename
from datasets import load_dataset, load_metric,concatenate_datasets,Dataset
dataset_name = "fleurs"
configs = ['fr_fr','de_de','nl_nl']
list_datasets_validation = []
for i in configs:   
    dataset_validation = load_dataset("google/fleurs",i,split = "validation")
    dataset_validation = Dataset.from_dict(dataset_validation[:20])
    list_datasets_validation.append(dataset_validation)
dataset_validation = concatenate_datasets(
        list_datasets_validation
    )
metric = load_metric("accuracy")
labels =["French","German","Dutch"]
label2id, id2label,label2id_int = dict(), dict(),dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label
    label2id_int[label] = i
from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
max_duration = 10.0  # seconds

"""We can then write the function that will preprocess our samples. We just feed them to the `feature_extractor` with the argument `truncation=True`, as well as the maximum sample length. This will ensure that very long inputs like the ones in the `_silence_` class can be safely batched."""

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays, 
        sampling_rate=feature_extractor.sampling_rate, 
        max_length=int(feature_extractor.sampling_rate * max_duration), 
        truncation=True, 
    )
    inputs["labels"] = [label2id_int[image] for image in examples["language"]]
    return inputs
encoded_dataset_validation = dataset_validation.map(preprocess_function, remove_columns=["id","num_samples", "path", "audio", "transcription", "raw_transcription", "gender", "lang_id", "language", "lang_group_id"], batched=True)

dataset_name_o = "multilingual_librispeech"
configs_o = ['french', 'german', 'dutch']
list_datasets_validation_o = []
for i in configs_o:   
    dataset_validation = load_dataset("facebook/multilingual_librispeech",i,split = "train.1h")
    dataset_validation = Dataset.from_dict(dataset_validation[:20])
    list_datasets_validation_o.append(dataset_validation)
dataset_validation_o = concatenate_datasets(
        list_datasets_validation_o
    )


"""We can then write the function that will preprocess our samples. We just feed them to the `feature_extractor` with the argument `truncation=True`, as well as the maximum sample length. This will ensure that very long inputs like the ones in the `_silence_` class can be safely batched."""

def preprocess_function_o(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays, 
        sampling_rate=feature_extractor.sampling_rate, 
        max_length=int(feature_extractor.sampling_rate * max_duration), 
        truncation=True, 
    )
    return inputs
encoded_dataset_validation_o = dataset_validation_o.map(preprocess_function_o, remove_columns=['file','audio','text','speaker_id','chapter_id','id'], batched=True)

from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

model_name_extension = "".join(configs)
model_name = model_checkpoint.split("/")[-1]+model_name_extension+dataset_name


import numpy as np

def compute_metrics(eval_pred):
    """Computes accuracy on a batch of predictions"""
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)


best_model = AutoModelForAudioClassification.from_pretrained(
    model_checkpoint
)
args = best_model.training_args
trainer = Trainer(
    best_model,
    args,
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics
)
# for batch in trainer.get_train_dataloader():
#     break
# from scipy.io.wavfile import write
# write('output_sounddevice.wav', 16000, np.array(batch["input_values"][0]))
# print(f"after loading model:{trainer.evaluate()}")
pred_o= trainer.predict(encoded_dataset_validation_o)
print(f"original_accuracy:{pred_o}")
pred= trainer.predict(encoded_dataset_validation)
print(f"fleaurs_accuracy:{pred}")
best_model = best_model.wav2vec2

inp = encoded_dataset_validation_o["input_values"][::2]
labels_p = encoded_dataset_validation_o["labels"][::2]
pred = best_model(torch.tensor(inp))
pred = pred.last_hidden_state.reshape(pred.last_hidden_state.shape[0],-1)
pred = pred.detach().numpy()
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pred =pca.fit_transform(pred)

import numpy as np
import matplotlib.pyplot as plt


fig, ax = plt.subplots()
 
# Plot scaled features
xdata = pred[:,0]
ydata = pred[:,1]
 
# Plot 3D plot
scatter =ax.scatter(xdata, ydata,c=labels_p)
 
# Plot title of graph
plt.title(f'PCA of original')
ax.legend(handles=scatter.legend_elements()[0],labels=labels)
plt.show()

print("end")

inp = encoded_dataset_validation["input_values"][::2]
labels_p = encoded_dataset_validation["labels"][::2]
pred = best_model(torch.tensor(inp))
pred = pred.last_hidden_state.reshape(pred.last_hidden_state.shape[0],-1)
pred = pred.detach().numpy()
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pred =pca.fit_transform(pred)

import numpy as np
import matplotlib.pyplot as plt


fig, ax = plt.subplots()
 
# Plot scaled features
xdata = pred[:,0]
ydata = pred[:,1]
 
# Plot 3D plot
scatter =ax.scatter(xdata, ydata,c=labels_p)
 
# Plot title of graph
plt.title(f'PCA of fleurs')
ax.legend(handles=scatter.legend_elements()[0],labels=labels)
plt.show()

print("end")
